How to code in 2022 with tips on continuous integration, static/runtime checks, etc
    https://www.youtube.com/watch?v=q7Gv4J3FyYE&ab_channel=C%2B%2BWeeklyWithJasonTurner

TODO:
    Turn on more warnings and fix them:
        -Wextra
        -Wshadow
        -Wconversion
        -Wpedantic
        -Werror
        -fanalyzer (gcc)
        clang-tidy
        sanitizers:
            address
            undefined behavior
            memory
            thread

    test with different compilers (clang and gcc)
        different compilers give different warnings

    Read Chapter 8, 9, and 10 in java book (in fact, read it all.  Most of it seems really good)
        Chapter 2.
            make shared library for weasel db a C library (rather than wsldb in golang now)
            golang client should then use the wsldbc API to connect to database system
            wsldbc API (weaselDB connectivity API)
                Driver
                Connection (also can turn off autocommit, commit, rollback, set isolation level)
                Statement   
                ResultSet
                ResultSetMetaData 
        Chapter 4. Memory Management
            Log Manager <--------------Look here for info on how logging works
        Chapter 8. Query Processing
            Replace interpreter/WorkTable with:
                SelectScan(input table, predicate) (updateable)
                ProductScan(table1, table2)
                ProjectScan(input table, field names)
                TableScan(table name) (updateable)
            
                Note: SelectScan and TableScan are updateable (eg, can replace record fields)
    
            Seems like pipelining is mostly effective, but materialization can optimize
            in certain cases (mentions a later chapter discusses this)

        Chapter 10. Planner <------------------START READING HERE
            Verify (should change our term 'Analyze' to 'Verify')
            
    Transactions round 2
        Two transaction tests still fail <------------------------------------------START HERE
            In the previous version we aborted a txn immediately on failure and skipped stmts until commit/rollback
            but this shouldn't happen now since users can enter single stmts at a time
            update tests so that ALL queries are run until commit/rollback is called

            Here's the way postgres does it:
                If a transaction is aborted (eg, any query fails or even invalid syntax), it will ignore all queries until commit/rollback is called
                'commit' will rollback if any queries failed in txn, and only commit if no errors during txn
                !!! Could keep a variable in Txn class called has_aborted = false, and set to true if any stmt execution fails
                    when commit is called, rollback if true, commit if false.  rollback statement will always call rollback
                    if has_aborted is true, then skip analyzing/executing statements until 'commit'/'rollback' is called

        Write test client that interleaves queries in two transactions
            Need to test transactions by interleaving sql queries, but right now each SQL statement can't be processed

        should set timeout for txns so that in case the client crashes with on open txn it doesn't
            take up server resources. 
            Will also prevent locking when a client opens a transaction for a long time
        
        Add mutexes to lock database when creating/dropping tables

        GetColFamIdx and GetColFamHandle (both of them) can probably be merged

        Are ColumnFamilyDescriptors necessary?  Only needed when opening database
            If not needed, remove as member variable from storage.h

    Make testing code less brittle and embedded directly into code rather than in separate files (.sql and .exp)
        should make an embedded SQL driver so this works:
            resultset[0].assertEq("cat"); //makes it easier to test

    Make website for WeaselDB (Flipping it upside down gives us M......l - Machine Learning!!!)

    Planner - should choose join types, scan types, etc based on some criteria
        planner should go between the analyze and executor stages

    WorkTable and Scans should be different things:
        WorkTable
            Joins ...
            Constant
            Table

        Scan
            TableScan - scan entire work table
            SelectScan - filter rows while scanning work table
            ProductScan - scan each row in one table with each row in other
            ProjectScan - removes some columns from work table
            
        Scans and Joins are different:
            A Scan is a method of grabbing data from a table
                SelectScan (filters out rows that don't meet some criteria)
                TableScan (reads all rows from a table)
                ProductScan (reads in two tables)
                ProjectScan (reads in table and filters out columns)

    Add in machine learning
        Add to Makefile to allow compilation with/without pytorch (without it compiles faster)
        Need to be able to link dynamically since torchlib requires the model defined in c++
    
        Syntax:
            create model my_network from 'nn.pt';
            train my_network on (select m.target, m.pixels from mnist_train as m); //first column is target
            select my_network(m.pixels), m.target from mnist_test as m;

    Rather than having a golang client library, write it in C or C++ and put it directly into wsldb
        The golang client can then use this library

    Tests for errors
        How can we test whether errors are being caught?  Could just have a sequence of tests with invalid SQL
            and make sure first 6 characters of output are 'Error:'...?
            write a script that will trigger each error - need to be comprehensive 
            make a new directory called error with all tests to catch errors (just check that 'Error:' is the first string
            Should maybe make 3 error types: 'Tokenize Error:', 'Parse Error:', 'Execution Error:'
            Put these tests inside of test/error, but have the test.py script also run these
            Call the tests tokenize_1.err, tokenize_2.err and tokenize_1.exp, tokenize_2.exp (but just check if first part of string is correct)

    Start using smart pointers where necessary:
        start with unique_ptr that will free memory when going out of scope (or when object storing it goes out of scope)
        we are passing a lot of pointers to constructors - what's the best way to do this?
        Make smart pointer and just pass to classes.  If a class doesn't have ownership, make it a raw pointer (Eg, Expr and Stmt have raw pointer to QueryState)

    All identifiers/keywords (not strings) should be lowercased to so that uppercase is accepted (like postgresql)

    Entering a single ; (semicolon) from client causes server to crash - this isn't good
        Should just end immediately since it's a valid query (not a useful, but valid)

    Integrate mnist model into wsldb
        make a table to store mnist data as bytes:
            create table numbers (pixels bytea);
            insert into numbers (pixels) values ('\xff');
            insert into numbers (pixels) values (wsldb_read_binary_final('./data/test1')), (wsldb_read_binary_file('./data/test2'));
            select mnist_model(n.pixels) from numbers; //will need to rewrite dataset class to only read in the 28x28 data and no target

        Do sentiment analysis using a linear network for now just to test
            create table comments (comment text);
            //insert comments into table here
            select sentiment_model(c.comment) from comments;

        test generating input data from a list of 
            description of file format:
                https://rstudio-pubs-static.s3.amazonaws.com/465274_cd4d339c79aa4d94a0140b63aaf065e0.html
            How can we make a data loader for individual pixels (28 x 28)????
            Test this in main.cc in pytorch_cpp_test
            store a single image as database record (target + data). target is null if not provided.
            select ffnn(m.data) = m.target from mnist as m;
        If something is predicted, database should read in entire columnn and pass that into model for prediction
            (rather than passing single records in at a time)
        replace loading of data directory with data directly inside the database
            load mnist dataset into database (load in test data/targets.  Use 'text' data type for database)
        lex/parse new keywords for this to work
            select my_model(m.pixels, m.rgbformat), answer from mnist_data as m;

    Make Golang backend that connects to wsldb (using atm locations db - will still use that example for testing system)
        Bug with transactions <---URGENT
            inserting a value, and then querying is buggy.  The inserted value in the same transactin doesn't show up
            test transaction_read_own_write.sql fails <---------fix wsldb transactions so this doesn't fail
        
        
        Use system design interview book and guide to building a system
            pick one with an SQL database included
            make the app using whatever tools
        https://deadsimplechat.com/blog/rest-api-with-golang-and-postgresql/

        Amazon EC2 linux virtual machine - see if I can host website on free tier here (either my wedding website or practice one)
            https://aws.amazon.com/getting-started/launch-a-virtual-machine-B-0/launch-a-virtual-machine-B-1/

    Use PostML syntax to train models directly on database data <------------------LOOK HERE FOR NEXT BIG STEP
        create model my_model using feed
        train my_model(m.pixels, m.rgbformat) to predict m.number from mnist_data as m;
        deploy my_model; //makes this model public on database for use with prediction
        select my_model(m.pixels, m.rgbformat), answer from mnist_data as m;

        sentiment analysis
            https://github.com/bentrevett/pytorch-sentiment-analysis/tree/master
            hard code a few sentiment models into the c++ codebase for now.  Create systems to call them like functions:
                select avg(sentiment_model(m.text)) from comments as m where m.data > 'September 20, 2020'; //get average sentiment after September 20, 2020
                select avg(sentiment_model(m.text)) from comments as m where m.user = 5; //get average sentiment for user with id = 5

                select p.first_name, p.last_name, kneighbor_model(p.age, p.sex, p.class) from persons where m.sex = 'Male'; //predict male deaths

        Use K-nearest neighbor on titanic dataset for testing

        Thesis: Putting model in database is faster.  Simplifies replication.  Simplifies data engineering pipeline.  Easier for analysts/data scientists.
        Return of the Monolith.
            1. Pull data from database and train model outside
            2. Train directly on the database system

    Errors round 2
        Since queries are quite short, rather than trying to pass back error messages in tokenizer/parser,
        just print out and error message and terminate (the backend thread should terminate, but the actual server should continue running)

    Transaction Isolation Round 2
        Use a single rocksdb for entire database (all tables/indexes share a single rocksdb database)
            this allows us to use the rocksdb transaction API to do atomic writes/reads
            There's still the problems of the catalog, but at least it allows serializable transactions
                within a database

        Write some concurrent code that breaks transaction isolation
            
        Even single table transactions are not done properly now
        Read-committed is the lowest isolation level I want to support, but even that's not working now
        Need to think of a situation where our current system fails (how can that happen?)

    Users and passwords
        Store hash of passwords

    SSL for logging into database

    Use ZooKeeper/Consul/Ectd to make the system distributed

    Secondary indexes
        Use atm locations database to test optimizer choices (eg, sequential vs index)

        Optmizer - where does this fit in?  What data does it need access to?
            what indexes are available?
            what columns does the 

            Sequential or index scan of each base relation
                sequential scan is default plan, and always a plan created
                index scan is used if 'attribute' op CONSTANT restriction in used in query
            Nested-loop, merge, hash join (if applicable)
            
        implement 'use/force index' (possible in MySQL) to see if secondary indexes can be used
            select system, name from planets force index secondary_idx_system_name;
        How do we even know if it's working?  Need a large dataset to test (maybe the atm one?)
        Could make an index on zip code, and then see if an index range scan on that index is faster then a simple table scan
        Or even just make sure that it's working.  Postgresql checks if the amount to scan is < 25%?, and only uses an index scan
            then, otherwise it does full table.  Having to do a second lookup

        startup cost + cost/row * rows to scan

    Need custom comparators for keys
        rocksdb is just using byte-level comparison now, but we need it compared using wsldb data types
        using secondary indexes without custom comparators wont' mean anything (but we are seeking to first entry now, so there's no difference yet)

    Transaction isolation, Round 3
        transactions are atomic with multiple writes within the same table, but NOT across tables (including to the catalogue)
        transaction manager needs to grab write locks on all tables that need to be written to, and then do an atomic write on 
            all of them.
        Need to take into consideration possibility of failure - do we need our own WAL here?   
        Or can TransactionDB API help here and do most of the heavy lifting for us?  It can provide snapshot isolation, read-commited isolation,
            and one more I didn't recognize

    Atomic writes across multiple rocksdb databases

    Isolation when multiple processes access same table(s)

    Query Optimizer
        Use the secondary indexes!
        Implement 'explain' and 'explain analyze' to see what the execution plan looks like

    Make wsldb a server database (need to do this to check if transactions run concurrently work)
        scheduler needs to receive SQL queries from server
        scheduler makes backend processes
        Will need to give wsldb a tcp server, and then write a simple client to send requests.  
        Can use Postgresql wire protocol: 1 byte is type (text or binary), next 4 bytes are message length (including length, but not type)
            and remaining bytes (length - 4) is the message. Just make the type text for now.  Read about how table info can be sent back
            to the client in that protocol

    Query optimization
        Use secondary indexes when possible (how do we know when it will be better?)
        total cost = startup cost + cost/row

    check constraints
    triggers
    group by
    having
    alter table (evolvability!)
    cascade
    manual secondary index creation
    CTE
    exceptions
    window functions
    views
    *
    show tables;
    arrays
    numeric
    create database/use/drop database/drop database if exists

Comments

Rocksdb uses LRU by default

How is a batch write and TransactionDB different in rocksdb?

Indices are not searchable by range, so will need a B+ tree for indexing still
    Read about how CockroachDB made their SQL implementation on rocksdb
        https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/
    Or can we provide rocksdb a custom comparator?

Insert records into secondary index
    key is whichever attribute is the selected index
    Need to concatenate primary key in case secondary index is not unique
        May need to provide custom comparison function here
    value is the primary key

Switch from raw pointers to smart pointers (unique should be sufficient for now)

Is making a B+ tree for indices still faster than relying on rocksdb indexes?

